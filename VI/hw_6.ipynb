{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/qbeer/07eb98879a555a676b6da86ea8cd7f9e/hw_6_raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Nr8CVzD4uC"
   },
   "source": [
    "## 1. Implement a linear model\n",
    "return the weight parameters w = (w1, w2, ... , wP) and the intercept parameter w0 separately where:\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "* check your returned coefficients with the built in `LinearRegression` class from the sklearn library, they should be within tolerance 1e-6 to each other\n",
    "\n",
    "* use a generated regression dataset from `sklearn.dataset import make_regression` API with parameters `n_samples=1000` and `n_features=20`\n",
    "\n",
    "## 2. Use of real data\n",
    "\n",
    "* download the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from UCI, the task includes understanding the dataset: naming the appropiate data fields, handling missing values, etc.\n",
    "\n",
    "* split the data in training/test sets and fit a LinearRegression model with 5-fold cross-validation on top of it - compare training and testing scores (R^2 by default) for the different CV splits, print the mean score and its standard deviation\n",
    "\n",
    "* fit the best Lasso regression model with 5-fold grid search cross validation (GridSearchCV) on the parameters: alpha, normalize, max_iter and show the best parameters\n",
    "\n",
    "## 3. Shrinkage\n",
    "\n",
    "* interpret Lasso models based on its descriptive parameters by the shrinkage method described during the lecture (make a plot and check the names of the features that are not eliminated by the penalty parameter) on the data we have here [ this is an explanatory data analysis problem, try to be creative ]\n",
    "\n",
    "* fit Ridge models and apply the shrinkage method as well, did you get what you expect?\n",
    "\n",
    "* do you think normalization is needed here? if so, do not forget to use it in the next tasks\n",
    "\n",
    "## 4. Subset selection\n",
    "\n",
    "* Split the data to training and test sets and do recursice feature elimination until 10 remaining predictors with 5-fold cross-validated regressors (RidgeCV, LassoCV, ElasticNetCV) on the training set, plot their names and look up some of their meanings [ recursive feature elimination is part of sklearn but you can do it with a for loop if you whish ]\n",
    "\n",
    "* Do all models provide the same descriptors? Check their performance on the test set! Plot all model predictions compared to the y_test on 3 different plots, which model seems to be the best?\n",
    "\n",
    "# 5. ElasticNet penalty surface\n",
    "* visualize the surface of the `objective(alpha, beta)`\n",
    "\n",
    " * parameters corresponding to the L1 and L2 regularizations. Select the best possible combination of the hyper-parameters that minimize the objective (clue: from scipy.optimize import minimize)\n",
    "\n",
    "* this task is similar to what you've seen during class, just not for MSE vs. single penalty parameter but MSE vs. two penalty parameters `alpha, beta`\n",
    "\n",
    "* interpret the overall results, do you think regularization is necessary at all? do you think linear models are powerful enough on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.datasets import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_selection import *\n",
    "from scipy.optimize import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwvvQJFCD8Eq"
   },
   "source": [
    "---------------------\n",
    "## 1. Implement a linear model\n",
    "return the weight parameters w = (w1, w2, ... , wP) and the intercept parameter w0 separately where:\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "* check your returned coefficients with the built in `LinearRegression` class from the sklearn library, they should be within tolerance 1e-6 to each other\n",
    "\n",
    "* use a generated regression dataset from `sklearn.dataset import make_regression` API with parameters `n_samples=1000` and `n_features=20`\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    _X    = np.concatenate((X, np.ones(shape=(X.shape[0], 1))),axis=1)\n",
    "    S     = _X.T@_X\n",
    "    S_inv = np.linalg.inv(S)\n",
    "    \n",
    "    unnormalized_weights = S_inv@ _X.T\n",
    "    weights              = np.dot(unnormalized_weights,y)\n",
    "    return weights[:-1], weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_regression(n_samples=1000,n_features=20)\n",
    "w,w0=linear_model(X,y)\n",
    "regr=LinearRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(regr.coef_,w,rtol=1e-8), 'The coefficients do not match'\n",
    "assert np.allclose(regr.intercept_,w0,rtol=1e-8), 'The intercept parameter does not mach'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## 2. Use of real data\n",
    "\n",
    "* download the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from UCI, the task includes understanding the dataset: naming the appropiate data fields, handling missing values, etc.\n",
    "\n",
    "* split the data in training/test sets and fit a LinearRegression model with 5-fold cross-validation on top of it - compare training and testing scores (R^2 by default) for the different CV splits, print the mean score and its standard deviation\n",
    "\n",
    "* fit the best Lasso regression model with 5-fold grid search cross validation (GridSearchCV) on the parameters: alpha, normalize, max_iter and show the best parameters\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(io.StringIO(requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\").content.decode('utf-8')), na_values='?')\n",
    "cols = [s.split(' ')[1] \n",
    "        for s in re.findall(r'@attribute.*',requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.names\").content.decode('utf-8'))]\n",
    "\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(20,5))\n",
    "ax.imshow(df.isna().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "mask=df.isna().values.sum(0)>=df.shape[0]/2\n",
    "df=df.drop(columns=df.columns[mask])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(20,5))\n",
    "ax.imshow(df.isna().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    print(df.columns.values[i],'---',df.dtypes.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communityname=df.pop('communityname')\n",
    "fold=df.pop('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.isna().values.all(): print('Number of missing values: '+ str(np.sum(df.isna().values)))\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "linreg  = LinearRegression()\n",
    "results = cross_validate(linreg,X,y,cv=5,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results.keys():\n",
    "    print(i,': \\n')\n",
    "    for j in range(5): print('  ',j+1,'-', results[i][j])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = np.asarray([np.mean(results['test_score']),np.std(results['test_score'])])\n",
    "train = np.asarray([np.mean(results['train_score']),np.std(results['train_score'])])\n",
    "\n",
    "print('training -- mean score and std: %.3f +/- %.3f' % (train[0],train[1]))\n",
    "print('test -- mean score and std: %.3f +/- %.3f' % (test[0],test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "\n",
    "dist = dict(alpha=np.linspace(1e-6, .05, 50),\n",
    "            normalize=[True, False], \n",
    "            max_iter=[10000, 50000])\n",
    "\n",
    "clf = GridSearchCV(lasso, dist, cv=5, verbose=1,\n",
    "                   return_train_score=True, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## 3. Shrinkage\n",
    "\n",
    "* interpret Lasso models based on its descriptive parameters by the shrinkage method described during the lecture (make a plot and check the names of the features that are not eliminated by the penalty parameter) on the data we have here [ this is an explanatory data analysis problem, try to be creative ]\n",
    "\n",
    "* fit Ridge models and apply the shrinkage method as well, did you get what you expect?\n",
    "\n",
    "* do you think normalization is needed here? if so, do not forget to use it in the next tasks\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c      = []\n",
    "alphas = np.linspace(1e-6,0.05,200)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha,max_iter=10000,normalize=True)\n",
    "    lasso.fit(X, y)\n",
    "    c.append(lasso.coef_)\n",
    "c=np.asarray(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2\n",
    "colorm=cm.gist_rainbow\n",
    "fig,ax=plt.subplots(1,1,figsize=(16,9))\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=c.shape[1])\n",
    "\n",
    "for i in range(c.shape[1]):\n",
    "    ax.plot(alphas,c[:,i], color=colorm(norm(i)))\n",
    "\n",
    "\n",
    "mask = np.where(c[x]!=0)[0]\n",
    "labels=df.columns.values[mask]\n",
    "\n",
    "ax.vlines(alphas[x], ymin=-0.6, ymax=0.6, \n",
    "          label='alpha = %.5f' % alphas[x], color='k',ls='--')\n",
    "ax.text(alphas[x], -0.61, '$\\\\alpha=$'+str(np.round(alphas[x],5)), fontsize=15)\n",
    "hand=[]\n",
    "for i,s in enumerate(labels):\n",
    "    hand.append(mpatches.Patch(color=colorm(norm(mask[i])), label=s+': %.3f'%c[x][mask[i]]))\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xscale('log')   \n",
    "ax.set_xlabel('Penalty parameter',fontsize=30)\n",
    "ax.set_ylabel('Coefficient',fontsize=30)\n",
    "ax.tick_params(labelsize=25)\n",
    "ax.legend(handles=hand, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c      = []\n",
    "alphas = np.linspace(1e-6, 1000, 200)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Ridge(alpha=alpha,\n",
    "                  max_iter=10_000,\n",
    "                  normalize=True)\n",
    "    lasso.fit(X, y)\n",
    "    c.append(lasso.coef_)\n",
    "\n",
    "c=np.asarray(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=10\n",
    "colorm=cm.gist_rainbow\n",
    "fig,ax=plt.subplots(1,1,figsize=(16,9))\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=c.shape[1])\n",
    "\n",
    "for i in range(c.shape[1]):\n",
    "    ax.plot(alphas,c[:,i], color=colorm(norm(i)))\n",
    "\n",
    "\n",
    "mask = np.where(c[x]!=0)[0]\n",
    "labels=df.columns.values[mask]\n",
    "\n",
    "ax.vlines(alphas[x], ymin=-0.6, ymax=0.6, \n",
    "          label='alpha = %.5f' % alphas[x], color='k',ls='--')\n",
    "ax.text(alphas[x], -0.61, '$\\\\alpha=$'+str(np.round(alphas[x],5)), fontsize=15)\n",
    "hand=[]\n",
    "for i,s in enumerate(labels):\n",
    "    hand.append(mpatches.Patch(color=colorm(norm(mask[i])), label=s+': %.3f'%c[x][mask[i]]))\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xscale('log')   \n",
    "ax.set_xlabel('Penalty parameter',fontsize=30)\n",
    "ax.set_ylabel('Coefficient',fontsize=30)\n",
    "ax.tick_params(labelsize=25)\n",
    "#ax.legend(handles=hand, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## 4. Subset selection\n",
    "\n",
    "* Split the data to training and test sets and do recursice feature elimination until 10 remaining predictors with 5-fold cross-validated regressors (RidgeCV, LassoCV, ElasticNetCV) on the training set, plot their names and look up some of their meanings [ recursive feature elimination is part of sklearn but you can do it with a for loop if you whish ]\n",
    "\n",
    "* Do all models provide the same descriptors? Check their performance on the test set! Plot all model predictions compared to the y_test on 3 different plots, which model seems to be the best?\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RidgeCV\n",
    "rfe = RFE(estimator=RidgeCV(cv=5, normalize=True), n_features_to_select=10)\n",
    "rfe = rfe.fit(X, y)\n",
    "features=pd.DataFrame(df.columns.values[:-1])\n",
    "\n",
    "var    = sorted(features[rfe.support_].values.tolist())\n",
    "var    = list(np.asarray(var).T[0])\n",
    "ridge_score  = rfe.score(X_test, y_test)\n",
    "ridge_pred = rfe.predict(X_test)\n",
    "\n",
    "result_data.append(var+[ridge_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoCV\n",
    "rfe = RFE(estimator=LassoCV(cv=5, normalize=True), n_features_to_select=10)\n",
    "rfe = rfe.fit(X, y)\n",
    "features=pd.DataFrame(df.columns.values[:-1])\n",
    "\n",
    "var    = sorted(features[rfe.support_].values.tolist())\n",
    "var    = list(np.asarray(var).T[0])\n",
    "lasso_score  = rfe.score(X_test, y_test)\n",
    "\n",
    "lasso_pred = rfe.predict(X_test)\n",
    "\n",
    "result_data.append(var+[lasso_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNetCV\n",
    "rfe = RFE(estimator=ElasticNetCV(cv=5, normalize=True), n_features_to_select=10)\n",
    "rfe = rfe.fit(X, y)\n",
    "features=pd.DataFrame(df.columns.values[:-1])\n",
    "\n",
    "var    = sorted(features[rfe.support_].values.tolist())\n",
    "var    = list(np.asarray(var).T[0])\n",
    "elastic_score  = rfe.score(X_test, y_test)\n",
    "elastic_pred = rfe.predict(X_test)\n",
    "\n",
    "result_data.append(var+[elastic_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(columns=['var_'+str(i) for i in range(1,11)]+['score'],\n",
    "                    index=['RidgeCV', 'LassoCV', 'ElasticNetCV'],\n",
    "                    data=result_data)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(18, 6))\n",
    "\n",
    "\n",
    "axes[0].plot(y_test, lasso_pred, ls='', marker='o',alpha=0.6, label='LassoCV--score = %.3f'%lasso_score)\n",
    "axes[1].plot(y_test, ridge_pred,  ls='', marker='o',alpha=0.6,label='RidgeCV--score = %.3f'%ridge_score )\n",
    "axes[2].plot(y_test, elastic_pred,  ls='', marker='o',alpha=0.6,label='ElasticNetCV--score = %.3f'% elastic_score)\n",
    "\n",
    "axes[0].set_ylabel('y$_{pred}$', fontsize=25)\n",
    "axes[1].set_xlabel('y$_{test}$', fontsize=25)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "    ax.set_xticks([0., .5, 1.])\n",
    "    ax.set_yticks([0., .5, 1.])\n",
    "    ax.plot(np.linspace(0, 1, 200), np.linspace(0, 1, 200), 'r-')\n",
    "    \n",
    "    \n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "# 5. ElasticNet penalty surface\n",
    "* visualize the surface of the `objective(alpha, beta)`\n",
    "\n",
    " * parameters corresponding to the L1 and L2 regularizations. Select the best possible combination of the hyper-parameters that minimize the objective (clue: from scipy.optimize import minimize)\n",
    "\n",
    "* this task is similar to what you've seen during class, just not for MSE vs. single penalty parameter but MSE vs. two penalty parameters `alpha, beta`\n",
    "\n",
    "* interpret the overall results, do you think regularization is necessary at all? do you think linear models are powerful enough on this dataset?\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha  = np.linspace(1e-6, 0.5, 100)\n",
    "beta   = np.linspace(0.1, 1, 100)\n",
    "scores = np.empty((100,100))\n",
    "\n",
    "for i,a in enumerate(alpha):\n",
    "    for j,b in enumerate(beta):\n",
    "        elastic = ElasticNet(alpha=a, l1_ratio=b, max_iter=15000, \n",
    "                             normalize=True).fit(X_train, y_train)\n",
    "        y_pred = elastic.predict(X_test)\n",
    "        mse=np.mean(np.square(y_pred - y_test))\n",
    "        scores[i][j]=mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCORE(param,X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    ''''''\n",
    "    a,b=param\n",
    "    elastic = ElasticNet(alpha=a, l1_ratio=b, max_iter=15000, \n",
    "                             normalize=True).fit(X_train, y_train)\n",
    "    y_pred = elastic.predict(X_test)\n",
    "    mse=np.mean(np.square(y_pred - y_test))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN = minimize(SCORE, x0=[1e-4, 1e-6], bounds=[(1e-10, 1), (1e-10, 1)]).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "alpha_grid, beta_grid = np.meshgrid(alpha, beta*alpha)\n",
    "\n",
    "ax.plot_surface(alpha_grid, beta_grid, scores, rstride=1, cstride=1,\n",
    "                       cmap=cm.cividis, linewidth=0, alpha=0.8, antialiased=False)\n",
    "\n",
    "\n",
    "ax.set_xlabel('L2', fontsize=25)\n",
    "ax.set_ylabel('L1', fontsize=25)\n",
    "ax.set_zlabel('MSE', fontsize=25)\n",
    "\n",
    "ax.plot(np.repeat(MIN[0], 100), np.repeat(MIN[1], 100), np.linspace(0, 0.05, 100), color='tab:red')\n",
    "ax.view_init(elev=20, azim=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "alpha_grid, beta_grid = np.meshgrid(alpha, beta*alpha)\n",
    "\n",
    "ax.plot_surface(alpha_grid, beta_grid, scores, rstride=1, cstride=1,\n",
    "                       cmap=cm.cividis, linewidth=0, alpha=0.8, antialiased=False)\n",
    "\n",
    "\n",
    "ax.set_xlabel('L2', fontsize=25)\n",
    "ax.set_ylabel('L1', fontsize=25)\n",
    "ax.set_zlabel('MSE', fontsize=25)\n",
    "\n",
    "ax.plot(np.repeat(min_penalty[0], 100), np.repeat(min_penalty[1], 100), np.linspace(0, 0.05, 100), color='tab:red')\n",
    "ax.view_init(elev=0, azim=-90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "alpha_grid, beta_grid = np.meshgrid(alpha, beta*alpha)\n",
    "\n",
    "ax.plot_surface(alpha_grid, beta_grid, scores, rstride=1, cstride=1,\n",
    "                       cmap=cm.cividis, linewidth=0, alpha=0.8, antialiased=False)\n",
    "\n",
    "\n",
    "ax.set_xlabel('L2', fontsize=25)\n",
    "ax.set_ylabel('L1', fontsize=25)\n",
    "ax.set_zlabel('MSE', fontsize=25)\n",
    "\n",
    "ax.plot(np.repeat(MIN[0], 100), np.repeat(MIN[1], 100), np.linspace(0, 0.05, 100), color='tab:red')\n",
    "ax.view_init(elev=20, azim=-120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1, figsize=(6,6))\n",
    "m=ax.imshow(scores, cmap=cm.viridis)\n",
    "fig.colorbar(m)\n",
    "ax.set_xlabel('$L_2$', fontsize=25)\n",
    "ax.set_ylabel('$L_1$', fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJfeC8q8fsR5o2JtAZDyNy",
   "include_colab_link": true,
   "name": "HW_6_raw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
